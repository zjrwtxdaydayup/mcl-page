





<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Python API and Gradio Frontend &mdash; mlc-llm 0.1.0 documentation</title>
  

  
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/tabs.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/tlcpack_theme.css" type="text/css" />

  
  

  
  
  
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
        <script src="../_static/tabs.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <script type="text/javascript" src="../_static/js/tlcpack_theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="iOS App and Swift API" href="ios.html" />
    <link rel="prev" title="CLI and C++ API" href="cli.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    
<header class="header">
    <div class="innercontainer">
      <div class="headerInner d-flex justify-content-between align-items-center">
          <div class="headerLogo">
          </div>

          <div id="headMenu" class="headerNav">
            <button type="button" id="closeHeadMenu" class="navCloseBtn"><img src="../_static/img/close-icon.svg" alt="Close"></button>
             <ul class="nav">
                <li class="nav-item">
                   <a class="nav-link" href=https://mlc.ai/mlc-llm>Home</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://github.com/mlc-ai/mlc-llm>Github</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://discord.gg/9Xpy2HGBuD>Discord Server</a>
                </li>
             </ul>
               <div class="responsivetlcdropdown">
                 <button type="button" class="btn-link">
                   Other Resources
                 </button>
                 <ul>
                     <li>
                       <a href=https://mlc.ai/>MLC Course</a>
                     </li>
                     <li>
                       <a href=https://mlc.ai/blog>MLC Blog</a>
                     </li>
                     <li>
                       <a href=https://webllm.mlc.ai/>Web LLM</a>
                     </li>
                 </ul>
               </div>
          </div>
            <div class="responsiveMenuIcon">
              <button type="button" id="menuBtn" class="btn-menu"><img src="../_static/img/menu-icon.svg" alt="Menu Icon"></button>
            </div>

            <div class="tlcDropdown">
              <div class="dropdown">
                <button type="button" class="btn-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                  Other Resources
                </button>
                <div class="dropdown-menu dropdown-menu-right">
                  <ul>
                     <li>
                       <a href=https://mlc.ai/>MLC Course</a>
                     </li>
                     <li>
                       <a href=https://mlc.ai/blog>MLC Blog</a>
                     </li>
                     <li>
                       <a href=https://webllm.mlc.ai/>Web LLM</a>
                     </li>
                  </ul>
                </div>
              </div>
          </div>
       </div>
    </div>
 </header>
 
    <nav data-toggle="wy-nav-shift" class="wy-nav-side fixed">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/mlc-logo-with-text-landscape.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
                <div class="version">
                  0.1.0
                </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../get_started/try_out.html">Try out MLC Chat</a></li>
<li class="toctree-l1"><a class="reference internal" href="../get_started/project_overview.html">Project Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../get_started/mlc_chat_config.html">Configure MLCChat in JSON</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Build and Deploy Apps</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="javascript.html">WebLLM and Javascript API</a></li>
<li class="toctree-l1"><a class="reference internal" href="rest.html">Rest API</a></li>
<li class="toctree-l1"><a class="reference internal" href="cli.html">CLI and C++ API</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Python API and Gradio Frontend</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#python-api">Python API</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#verify-installation">Verify Installation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#get-started">Get Started</a></li>
<li class="toctree-l3"><a class="reference internal" href="#tutorial-with-python-notebooks">Tutorial with Python Notebooks</a></li>
<li class="toctree-l3"><a class="reference internal" href="#configure-mlcchat-in-python">Configure MLCChat in Python</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#api-reference">API Reference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#mlc_chat.ChatModule"><code class="docutils literal notranslate"><span class="pre">ChatModule</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#mlc_chat.ChatModule.__init__"><code class="docutils literal notranslate"><span class="pre">ChatModule.__init__()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#mlc_chat.ChatModule.benchmark_generate"><code class="docutils literal notranslate"><span class="pre">ChatModule.benchmark_generate()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#mlc_chat.ChatModule.embed_text"><code class="docutils literal notranslate"><span class="pre">ChatModule.embed_text()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#mlc_chat.ChatModule.generate"><code class="docutils literal notranslate"><span class="pre">ChatModule.generate()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#mlc_chat.ChatModule.reset_chat"><code class="docutils literal notranslate"><span class="pre">ChatModule.reset_chat()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#mlc_chat.ChatModule.stats"><code class="docutils literal notranslate"><span class="pre">ChatModule.stats()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#gradio-frontend">Gradio Frontend</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="ios.html">iOS App and Swift API</a></li>
<li class="toctree-l1"><a class="reference internal" href="android.html">Android App</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Compile Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../compilation/compile_models.html">Compile Models via MLC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compilation/distribute_compiled_models.html">Distribute Compiled Models</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Define Model Architectures</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/customize/define_new_models.html">Define New Model Architectures</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Prebuilt Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../prebuilt_models.html">Model Prebuilts</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Dependency Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install/tvm.html">Install TVM Unity</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/conda.html">Install Conda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/gpu.html">GPU Drivers and SDKs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/emcc.html">Install Wasm Build Environment</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../community/guideline.html">Community Guideline</a></li>
<li class="toctree-l1"><a class="reference internal" href="../community/faq.html">Frequently Asked Questions</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      
      <nav class="wy-nav-top" aria-label="top navigation" data-toggle="wy-nav-top">
        
            <div class="togglemenu">

            </div>
            <div class="nav-content">
              <!-- mlc-llm -->
              Table of Contents
            </div>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        

          




















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> <span class="br-arrow">></span></li>
        
      <li>Python API and Gradio Frontend</li>
    
    
      
      
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/mlc-ai/mlc-llm/edit/main/docs/deploy/python.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="python-api-and-gradio-frontend">
<h1>Python API and Gradio Frontend<a class="headerlink" href="#python-api-and-gradio-frontend" title="Permalink to this heading">¶</a></h1>
<nav class="contents local" id="table-of-contents">
<p class="topic-title">Table of Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#python-api" id="id1">Python API</a></p>
<ul>
<li><p><a class="reference internal" href="#verify-installation" id="id2">Verify Installation</a></p></li>
<li><p><a class="reference internal" href="#get-started" id="id3">Get Started</a></p></li>
<li><p><a class="reference internal" href="#tutorial-with-python-notebooks" id="id4">Tutorial with Python Notebooks</a></p></li>
<li><p><a class="reference internal" href="#configure-mlcchat-in-python" id="id5">Configure MLCChat in Python</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#api-reference" id="id6">API Reference</a></p></li>
<li><p><a class="reference internal" href="#gradio-frontend" id="id7">Gradio Frontend</a></p></li>
</ul>
</nav>
<p>We expose Python API for the MLC-Chat for easy integration into other Python projects.
We also provide a web demo based on <a class="reference external" href="https://gradio.app/">gradio</a> as an example of using Python API to interact with MLC-Chat.</p>
<section id="python-api">
<h2><a class="toc-backref" href="#id1" role="doc-backlink">Python API</a><a class="headerlink" href="#python-api" title="Permalink to this heading">¶</a></h2>
<p>The Python API is a part of the MLC-Chat package, which we have prepared pre-built pip wheels and you can install it by
following the instructions in <a class="reference external" href="https://mlc.ai/package/">https://mlc.ai/package/</a>.</p>
<section id="verify-installation">
<h3><a class="toc-backref" href="#id2" role="doc-backlink">Verify Installation</a><a class="headerlink" href="#verify-installation" title="Permalink to this heading">¶</a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-c<span class="w"> </span><span class="s2">&quot;from mlc_chat import ChatModule; print(ChatModule)&quot;</span>
</pre></div>
</div>
<p>You are expected to see the information about the <a class="reference internal" href="#mlc_chat.ChatModule" title="mlc_chat.ChatModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_chat.ChatModule</span></code></a> class.</p>
<p>If the prebuilt is unavailable on your platform, or you would like to build a runtime
that supports other GPU runtime than the prebuilt version. Please refer our <a class="reference internal" href="rest.html#mlcchat-package-build-from-source"><span class="std std-ref">Build MLC-Chat Package From Source</span></a> tutorial.</p>
</section>
<section id="get-started">
<h3><a class="toc-backref" href="#id3" role="doc-backlink">Get Started</a><a class="headerlink" href="#get-started" title="Permalink to this heading">¶</a></h3>
<p>After confirming that the package <code class="docutils literal notranslate"><span class="pre">mlc_chat</span></code> is installed, we can follow the steps
below to chat with a MLC-compiled model in Python.</p>
<p>First, let us make sure that the MLC-compiled <code class="docutils literal notranslate"><span class="pre">model</span></code> we want to chat with already exists.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">model</span></code> has the format <code class="docutils literal notranslate"><span class="pre">f&quot;{model_name}-{quantize_mode}&quot;</span></code>. For instance, if
you used <code class="docutils literal notranslate"><span class="pre">q4f16_1</span></code> as the <code class="docutils literal notranslate"><span class="pre">quantize_mode</span></code> to compile <code class="docutils literal notranslate"><span class="pre">Llama-2-7b-chat-hf</span></code>, you
would have <code class="docutils literal notranslate"><span class="pre">model</span></code> being <code class="docutils literal notranslate"><span class="pre">Llama-2-7b-chat-hf-q4f16_1</span></code>.</p>
</div>
<p>If you do not have the MLC-compiled <code class="docutils literal notranslate"><span class="pre">model</span></code> ready:</p>
<ul class="simple">
<li><p>Checkout <a class="reference internal" href="../get_started/try_out.html#get-started"><span class="std std-ref">Try out MLC Chat</span></a> to download prebuilt models for simplicity, or</p></li>
<li><p>Checkout <a class="reference internal" href="../compilation/compile_models.html#compile-models-via-mlc"><span class="std std-ref">Compile Models via MLC</span></a> to compile models with <code class="docutils literal notranslate"><span class="pre">mlc_llm</span></code> (another package) yourself</p></li>
</ul>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-0-0-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-0-0-0" name="0-0" role="tab" tabindex="0">Check prebuilt models</button><button aria-controls="panel-0-0-1" aria-selected="false" class="sphinx-tabs-tab" id="tab-0-0-1" name="0-1" role="tab" tabindex="-1">Check compiled models</button></div><div aria-labelledby="tab-0-0-0" class="sphinx-tabs-panel" id="panel-0-0-0" name="0-0" role="tabpanel" tabindex="0"><p>If you downloaded prebuilt models from MLC LLM, by default:</p>
<ul class="simple">
<li><p>Model lib should be placed at <code class="docutils literal notranslate"><span class="pre">./dist/prebuilt/lib/$(model)-$(arch).$(suffix)</span></code>.</p></li>
<li><p>Model weights and chat config are located under <code class="docutils literal notranslate"><span class="pre">./dist/prebuilt/mlc-chat-$(model)/</span></code>.</p></li>
</ul>
<details class="summary-example">
<summary>Example</summary><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt;<span class="w"> </span>ls<span class="w"> </span>-l<span class="w"> </span>./dist/prebuilt/lib
Llama-2-7b-chat-hf-q4f16_1-metal.so<span class="w">  </span><span class="c1"># Format: $(model)-$(arch).$(suffix)</span>
Llama-2-7b-chat-hf-q4f16_1-vulkan.so
...
&gt;&gt;&gt;<span class="w"> </span>ls<span class="w"> </span>-l<span class="w"> </span>./dist/prebuilt/mlc-chat-Llama-2-7b-chat-hf-q4f16_1<span class="w">  </span><span class="c1"># Format: ./dist/prebuilt/mlc-chat-$(model)/</span>
<span class="c1"># chat config:</span>
mlc-chat-config.json
<span class="c1"># model weights:</span>
ndarray-cache.json
params_shard_*.bin
...
</pre></div>
</div>
</details></div><div aria-labelledby="tab-0-0-1" class="sphinx-tabs-panel" hidden="true" id="panel-0-0-1" name="0-1" role="tabpanel" tabindex="0"><p>If you have compiled models using MLC LLM, by default:</p>
<ul class="simple">
<li><p>Model libraries should be placed at <code class="docutils literal notranslate"><span class="pre">./dist/$(model)/$(model)-$(arch).$(suffix)</span></code>.</p></li>
<li><p>Model weights and chat config are located under <code class="docutils literal notranslate"><span class="pre">./dist/$(model)/params/</span></code>.</p></li>
</ul>
<details class="summary-example">
<summary>Example</summary><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt;<span class="w"> </span>ls<span class="w"> </span>-l<span class="w"> </span>./dist/Llama-2-7b-chat-hf-q4f16_1/<span class="w"> </span><span class="c1"># Format: ./dist/$(model)/</span>
Llama-2-7b-chat-hf-q4f16_1-metal.so<span class="w">  </span><span class="c1"># Format: $(model)-$(arch).$(suffix)</span>
...
&gt;&gt;&gt;<span class="w"> </span>ls<span class="w"> </span>-l<span class="w"> </span>./dist/Llama-2-7b-chat-hf-q4f16_1/params<span class="w">  </span><span class="c1"># Format: ``./dist/$(model)/params/``</span>
<span class="c1"># chat config:</span>
mlc-chat-config.json
<span class="c1"># model weights:</span>
ndarray-cache.json
params_shard_*.bin
...
</pre></div>
</div>
</details></div></div>
<p>After making sure that the files exist, using the conda environment you used
to install <code class="docutils literal notranslate"><span class="pre">mlc_chat</span></code>, from the <code class="docutils literal notranslate"><span class="pre">mlc-llm</span></code> directory, you can create a Python
file <code class="docutils literal notranslate"><span class="pre">sample_mlc_chat.py</span></code> and paste the following lines:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlc_chat</span> <span class="kn">import</span> <span class="n">ChatModule</span>
<span class="kn">from</span> <span class="nn">mlc_chat.callback</span> <span class="kn">import</span> <span class="n">StreamToStdout</span>

<span class="c1"># From the mlc-llm directory, run</span>
<span class="c1"># $ python sample_mlc_chat.py</span>

<span class="c1"># Create a ChatModule instance</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">ChatModule</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;Llama-2-7b-chat-hf-q4f16_1&quot;</span><span class="p">)</span>
<span class="c1"># You can change to other models that you downloaded, for example,</span>
<span class="c1"># cm = ChatModule(model=&quot;Llama-2-13b-chat-hf-q4f16_1&quot;)  # Llama2 13b model</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
   <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;What is the meaning of life?&quot;</span><span class="p">,</span>
   <span class="n">progress_callback</span><span class="o">=</span><span class="n">StreamToStdout</span><span class="p">(</span><span class="n">callback_interval</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>

<span class="c1"># Print prefill and decode performance statistics</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Statistics: </span><span class="si">{</span><span class="n">cm</span><span class="o">.</span><span class="n">stats</span><span class="p">()</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
   <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;How many points did you list out?&quot;</span><span class="p">,</span>
   <span class="n">progress_callback</span><span class="o">=</span><span class="n">StreamToStdout</span><span class="p">(</span><span class="n">callback_interval</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>

<span class="c1"># Reset the chat module by</span>
<span class="c1"># cm.reset_chat()</span>
</pre></div>
</div>
<p>Now run the Python file to start the chat</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>sample_mlc_chat.py
</pre></div>
</div>
<p>You can also checkout the <a class="reference internal" href="../prebuilt_models.html"><span class="doc">Model Prebuilts</span></a> page to run other models.</p>
<details class="summary-see-output">
<summary>See output</summary><div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Using model folder: ./dist/prebuilt/mlc-chat-Llama-2-7b-chat-hf-q4f16_1
Using mlc chat config: ./dist/prebuilt/mlc-chat-Llama-2-7b-chat-hf-q4f16_1/mlc-chat-config.json
Using library model: ./dist/prebuilt/lib/Llama-2-7b-chat-hf-q4f16_1-cuda.so

Thank you for your question! The meaning of life is a complex and subjective topic that has been debated by philosophers, theologians, scientists, and many others for centuries. There is no one definitive answer to this question, as it can vary depending on a person&#39;s beliefs, values, experiences, and perspectives.

However, here are some possible ways to approach the question:

1. Religious or spiritual beliefs: Many people believe that the meaning of life is to fulfill a divine or spiritual purpose, whether that be to follow a set of moral guidelines, to achieve spiritual enlightenment, or to fulfill a particular destiny.
2. Personal growth and development: Some people believe that the meaning of life is to learn, grow, and evolve as individuals, to develop one&#39;s talents and abilities, and to become the best version of oneself.
3. Relationships and connections: Others believe that the meaning of life is to form meaningful connections and relationships with others, to love and be loved, and to build a supportive and fulfilling social network.
4. Contribution and impact: Some people believe that the meaning of life is to make a positive impact on the world, to contribute to society in a meaningful way, and to leave a lasting legacy.
5. Simple pleasures and enjoyment: Finally, some people believe that the meaning of life is to simply enjoy the present moment, to find pleasure and happiness in the simple things in life, and to appreciate the beauty and wonder of the world around us.

Ultimately, the meaning of life is a deeply personal and subjective question, and each person must find their own answer based on their own beliefs, values, and experiences.

Statistics: prefill: 3477.5 tok/s, decode: 153.6 tok/s

I listed out 5 possible ways to approach the question of the meaning of life.
</pre></div>
</div>
</details><div class="line-block">
<div class="line"><br /></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You could also specify the address of <code class="docutils literal notranslate"><span class="pre">model</span></code> and <code class="docutils literal notranslate"><span class="pre">lib_path</span></code> explicitly. If
you only specify <code class="docutils literal notranslate"><span class="pre">model</span></code> as <code class="docutils literal notranslate"><span class="pre">model_name</span></code> and <code class="docutils literal notranslate"><span class="pre">quantize_mode</span></code>, we will
do a search for you. See more in the documentation of <a class="reference internal" href="#mlc_chat.ChatModule.__init__" title="mlc_chat.ChatModule.__init__"><code class="xref py py-meth docutils literal notranslate"><span class="pre">mlc_chat.ChatModule.__init__()</span></code></a>.</p>
</div>
</section>
<section id="tutorial-with-python-notebooks">
<h3><a class="toc-backref" href="#id4" role="doc-backlink">Tutorial with Python Notebooks</a><a class="headerlink" href="#tutorial-with-python-notebooks" title="Permalink to this heading">¶</a></h3>
<p>Now that you have tried out how to chat with the model in Python, we would
recommend you to checkout the following tutorials in Python notebook (all runnable in Colab):</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/mlc-ai/notebooks/blob/main/mlc-llm/tutorial_chat_module_getting_started.ipynb">Getting Started with MLC-LLM</a>:
how to quickly download prebuilt models and chat with it</p></li>
<li><p><a class="reference external" href="https://github.com/mlc-ai/notebooks/blob/main/mlc-llm/tutorial_compile_llama2_with_mlc_llm.ipynb">Compiling Llama-2 with MLC-LLM</a>:
how to use Python APIs to compile models with the MLC-LLM workflow</p></li>
<li><p><a class="reference external" href="https://github.com/mlc-ai/notebooks/blob/main/mlc-llm/tutorial_extensions_to_more_model_variants.ipynb">Extensions to More Model Variants</a>:
how to use Python APIs to compile and chat with any model variant you’d like</p></li>
</ul>
</section>
<section id="configure-mlcchat-in-python">
<h3><a class="toc-backref" href="#id5" role="doc-backlink">Configure MLCChat in Python</a><a class="headerlink" href="#configure-mlcchat-in-python" title="Permalink to this heading">¶</a></h3>
<p>If you have checked out <a class="reference internal" href="../get_started/mlc_chat_config.html#configure-mlc-chat-json"><span class="std std-ref">Configure MLCChat in JSON</span></a>, you would know
that you could configure MLCChat through various fields such as <code class="docutils literal notranslate"><span class="pre">temperature</span></code>. We provide the
option of overriding any field you’d like in Python, so that you do not need to manually edit
<code class="docutils literal notranslate"><span class="pre">mlc-chat-config.json</span></code>.</p>
<p>Since there are two concepts – <cite>MLCChat Configuration</cite> and <cite>Conversation Configuration</cite> – we correspondingly
provide two dataclasses <code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_chat.ChatConfig</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_chat.ConvConfig</span></code>.</p>
<p>We provide an example below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlc_chat</span> <span class="kn">import</span> <span class="n">ChatModule</span><span class="p">,</span> <span class="n">ChatConfig</span><span class="p">,</span> <span class="n">ConvConfig</span>
<span class="kn">from</span> <span class="nn">mlc_chat.callback</span> <span class="kn">import</span> <span class="n">StreamToStdout</span>

<span class="c1"># Using a `ConvConfig`, we modify `system`, a field in the conversation template</span>
<span class="c1"># `system` refers to the prompt encoded before starting the chat</span>
<span class="n">conv_config</span> <span class="o">=</span> <span class="n">ConvConfig</span><span class="p">(</span><span class="n">system</span><span class="o">=</span><span class="s1">&#39;Please show as much happiness as you can when talking to me.&#39;</span><span class="p">)</span>

<span class="c1"># We then include the `ConvConfig` instance in `ChatConfig` while overriding `max_gen_len`</span>
<span class="c1"># Note that `conv_config` is an optional subfield of `chat_config`</span>
<span class="n">chat_config</span> <span class="o">=</span> <span class="n">ChatConfig</span><span class="p">(</span><span class="n">max_gen_len</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">conv_config</span><span class="o">=</span><span class="n">conv_config</span><span class="p">)</span>

<span class="c1"># Using the `chat_config` we created, instantiate a `ChatModule`</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">mlc_chat</span><span class="o">.</span><span class="n">ChatModule</span><span class="p">(</span><span class="s1">&#39;Llama-2-7b-chat-hf-q4f16_1&#39;</span><span class="p">,</span> <span class="n">chat_config</span><span class="o">=</span><span class="n">chat_config</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
   <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;What is one plus one?&quot;</span><span class="p">,</span>
   <span class="n">progress_callback</span><span class="o">=</span><span class="n">StreamToStdout</span><span class="p">(</span><span class="n">callback_interval</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>

<span class="c1"># You could also pass in a `ConvConfig` instance to `reset_chat()`</span>
<span class="n">conv_config</span> <span class="o">=</span> <span class="n">ConvConfig</span><span class="p">(</span><span class="n">system</span><span class="o">=</span><span class="s1">&#39;Please show as much sadness as you can when talking to me.&#39;</span><span class="p">)</span>
<span class="n">chat_config</span> <span class="o">=</span> <span class="n">ChatConfig</span><span class="p">(</span><span class="n">max_gen_len</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">conv_config</span><span class="o">=</span><span class="n">conv_config</span><span class="p">)</span>
<span class="n">cm</span><span class="o">.</span><span class="n">reset_chat</span><span class="p">(</span><span class="n">chat_config</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
   <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;What is one plus one?&quot;</span><span class="p">,</span>
   <span class="n">progress_callback</span><span class="o">=</span><span class="n">StreamToStdout</span><span class="p">(</span><span class="n">callback_interval</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
<details class="summary-see-output">
<summary>See output</summary><div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Using model folder: ./dist/prebuilt/mlc-chat-Llama-2-7b-chat-hf-q4f16_1
Using mlc chat config: ./dist/prebuilt/mlc-chat-Llama-2-7b-chat-hf-q4f16_1/mlc-chat-config.json
Using library model: ./dist/prebuilt/lib/Llama-2-7b-chat-hf-q4f16_1-cuda.so

Oh, wow, *excitedly* one plus one? *grinning* Well, let me see... *counting on fingers* One plus one is... *eureka* Two!
...

*Sobs* Oh, the tragedy of it all... *sobs* One plus one... *chokes back tears* It&#39;s... *gulps* it&#39;s... *breaks down in tears* TWO!
...
</pre></div>
</div>
</details><div class="line-block">
<div class="line"><br /></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You do not need to specify the entire <code class="docutils literal notranslate"><span class="pre">ChatConfig</span></code> or <code class="docutils literal notranslate"><span class="pre">ConvConfig</span></code>. Instead, we will first
load all the fields defined in <code class="docutils literal notranslate"><span class="pre">mlc-chat-config.json</span></code>, a file required when instantiating
a <a class="reference internal" href="#mlc_chat.ChatModule" title="mlc_chat.ChatModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_chat.ChatModule</span></code></a>. Then, we will load in the optional <code class="docutils literal notranslate"><span class="pre">ChatConfig</span></code> you provide, overriding the
fields specified.</p>
<p>It is also worth noting that <code class="docutils literal notranslate"><span class="pre">ConvConfig</span></code> itself is overriding the original conversation template
specified by the field <code class="docutils literal notranslate"><span class="pre">conv_template</span></code> in chat configuration. Learn more about it in
<a class="reference internal" href="../get_started/mlc_chat_config.html#configure-mlc-chat-json"><span class="std std-ref">Configure MLCChat in JSON</span></a>.</p>
</div>
</section>
</section>
<section id="api-reference">
<h2><a class="toc-backref" href="#id6" role="doc-backlink">API Reference</a><a class="headerlink" href="#api-reference" title="Permalink to this heading">¶</a></h2>
<p>User can initiate a chat module by creating <a class="reference internal" href="#mlc_chat.ChatModule" title="mlc_chat.ChatModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_chat.ChatModule</span></code></a> class, which is a wrapper of the MLC-Chat model.
The <a class="reference internal" href="#mlc_chat.ChatModule" title="mlc_chat.ChatModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_chat.ChatModule</span></code></a> class provides the following methods:</p>
<dl class="py class">
<dt class="sig sig-object py" id="mlc_chat.ChatModule">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mlc_chat.</span></span><span class="sig-name descname"><span class="pre">ChatModule</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chat_config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">ChatConfig</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lib_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlc_chat.ChatModule" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>The ChatModule for MLC LLM.</p>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlc_chat</span> <span class="kn">import</span> <span class="n">ChatModule</span>
<span class="kn">from</span> <span class="nn">mlc_chat.callback</span> <span class="kn">import</span> <span class="n">StreamToStdout</span>

<span class="c1"># Create a ChatModule instance</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">ChatModule</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;Llama-2-7b-chat-hf-q4f16_1&quot;</span><span class="p">)</span>

<span class="c1"># Generate a response for a given prompt</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;What is the meaning of life?&quot;</span><span class="p">,</span>
    <span class="n">progress_callback</span><span class="o">=</span><span class="n">StreamToStdout</span><span class="p">(</span><span class="n">callback_interval</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>

<span class="c1"># Print prefill and decode performance statistics</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Statistics: </span><span class="si">{</span><span class="n">cm</span><span class="o">.</span><span class="n">stats</span><span class="p">()</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;How many points did you list out?&quot;</span><span class="p">,</span>
    <span class="n">progress_callback</span><span class="o">=</span><span class="n">StreamToStdout</span><span class="p">(</span><span class="n">callback_interval</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>str</em>) – The model folder after compiling with MLC-LLM build process. The parameter
can either be the model name with its quantization scheme
(e.g. <code class="docutils literal notranslate"><span class="pre">Llama-2-7b-chat-hf-q4f16_1</span></code>), or a full path to the model
folder. In the former case, we will use the provided name to search
for the model folder over possible paths.</p></li>
<li><p><strong>device</strong> (<em>str</em>) – The description of the device to run on. User should provide a string in the
form of ‘device_name:device_id’ or ‘device_name’, where ‘device_name’ is one of
‘cuda’, ‘metal’, ‘vulkan’, ‘rocm’, ‘opencl’, ‘auto’ (automatically detect the
local device), and ‘device_id’ is the device id to run on. If no ‘device_id’
is provided, it will be set to 0 by default.</p></li>
<li><p><strong>chat_config</strong> (<em>Optional</em><em>[</em><em>ChatConfig</em><em>]</em>) – A <code class="docutils literal notranslate"><span class="pre">ChatConfig</span></code> instance partially filled. Will be used to override the
<code class="docutils literal notranslate"><span class="pre">mlc-chat-config.json</span></code>.</p></li>
<li><p><strong>lib_path</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – The full path to the model library file to use (e.g. a <code class="docutils literal notranslate"><span class="pre">.so</span></code> file).
If unspecified, we will use the provided <code class="docutils literal notranslate"><span class="pre">model</span></code> to search over
possible paths.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="mlc_chat.ChatModule.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chat_config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">ChatConfig</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lib_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlc_chat.ChatModule.__init__" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mlc_chat.ChatModule.benchmark_generate">
<span class="sig-name descname"><span class="pre">benchmark_generate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prompt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generate_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#mlc_chat.ChatModule.benchmark_generate" title="Permalink to this definition">¶</a></dt>
<dd><p>Controlled generation with input prompt and fixed number of
generated tokens, ignoring system prompt. For example,</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlc_chat</span> <span class="kn">import</span> <span class="n">ChatModule</span>

<span class="n">cm</span> <span class="o">=</span> <span class="n">ChatModule</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;Llama-2-7b-chat-hf-q4f16_1&quot;</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">benchmark_generate</span><span class="p">(</span><span class="s2">&quot;What&#39;s the meaning of life?&quot;</span><span class="p">,</span> <span class="n">generate_length</span><span class="o">=</span><span class="mi">256</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Generated text:</span><span class="se">\n</span><span class="si">{</span><span class="n">output</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Statistics: </span><span class="si">{</span><span class="n">cm</span><span class="o">.</span><span class="n">stats</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>will generate 256 tokens in total based on prompt “What’s the meaning
of life?”. After generation, you can use <cite>cm.stats()</cite> to print the
generation speed.</p>
<p class="rubric">Notes</p>
<p>1. This function is typically used in controlled benchmarks. It generates
text without system prompt (i.e., it is pure text generation with no chat
style) and ignores the token stop model(s).
2. To make the benchmark as accurate as possible, we first do a round of
warmup prefill and decode before text generation.
3. This function resets the previous performance statistics.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>prompt</strong> (<em>str</em>) – The prompt of the text generation.</p></li>
<li><p><strong>generate_length</strong> (<em>int</em>) – The target length of generation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>output</strong> – The generated text output.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mlc_chat.ChatModule.embed_text">
<span class="sig-name descname"><span class="pre">embed_text</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlc_chat.ChatModule.embed_text" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a text input, returns its embedding in the LLM.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input</strong> (<em>str</em>) – The user input string.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>embedding</strong> – The embedding of the text.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>tvm.runtime.NDArray</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This is a high-level method and is only used for retrieving text embeddings. Users are
not supposed to call <a class="reference internal" href="#mlc_chat.ChatModule.generate" title="mlc_chat.ChatModule.generate"><code class="xref py py-func docutils literal notranslate"><span class="pre">generate()</span></code></a> after calling this method in the same chat session,
since the input to this method is not prefilled and will cause error. If user needs to
call <a class="reference internal" href="#mlc_chat.ChatModule.generate" title="mlc_chat.ChatModule.generate"><code class="xref py py-func docutils literal notranslate"><span class="pre">generate()</span></code></a> later, please call <a class="reference internal" href="#mlc_chat.ChatModule.reset_chat" title="mlc_chat.ChatModule.reset_chat"><code class="xref py py-func docutils literal notranslate"><span class="pre">reset_chat()</span></code></a> first.
For a more fine-grained embedding API, see <code class="xref py py-func docutils literal notranslate"><span class="pre">_embed()</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mlc_chat.ChatModule.generate">
<span class="sig-name descname"><span class="pre">generate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prompt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">progress_callback</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#mlc_chat.ChatModule.generate" title="Permalink to this definition">¶</a></dt>
<dd><p>A high-level method that returns the full response from the chat module given a user prompt.
User can optionally specify which callback method to use upon receiving the response. By default,
no callback will be applied.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>prompt</strong> (<em>str</em>) – The user input prompt, i.e. a question to ask the chat module.</p></li>
<li><p><strong>progress_callback</strong> (<em>object</em>) – The optional callback method used upon receiving a newly generated message from the chat module.
See <cite>mlc_chat/callback.py</cite> for a full list of available callback classes. Currently, only
streaming to stdout callback method is supported, see <cite>Examples</cite> for more detailed usage.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>output</strong> – The generated full output from the chat module.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>string</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Suppose we would like to stream the response of the chat module to stdout</span>
<span class="c1"># with a refresh interval of 2. Upon calling generate(), We will see the response of</span>
<span class="c1"># the chat module streaming to stdout piece by piece, and in the end we receive the</span>
<span class="c1"># full response as a single string `output`.</span>

<span class="kn">from</span> <span class="nn">mlc_chat</span> <span class="kn">import</span> <span class="n">ChatModule</span><span class="p">,</span> <span class="n">callback</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">ChatModule</span><span class="p">(</span><span class="n">xxx</span><span class="p">)</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;what&#39;s the color of banana?&quot;</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">callback</span><span class="o">.</span><span class="n">StreamToStdout</span><span class="p">(</span><span class="n">callback_interval</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mlc_chat.ChatModule.reset_chat">
<span class="sig-name descname"><span class="pre">reset_chat</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">chat_config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">ChatConfig</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlc_chat.ChatModule.reset_chat" title="Permalink to this definition">¶</a></dt>
<dd><p>Reset the chat session, clear all chat history, and potentially
override the original <cite>mlc-chat-config.json</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>chat_config</strong> (<em>Optional</em><em>[</em><em>ChatConfig</em><em>]</em>) – A <code class="docutils literal notranslate"><span class="pre">ChatConfig</span></code> instance partially filled. If specified, the chat
module will reload the <cite>mlc-chat-config.json</cite>, and override it with
<code class="docutils literal notranslate"><span class="pre">chat_config</span></code>, just like in initialization.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The model remains the same after <a class="reference internal" href="#mlc_chat.ChatModule.reset_chat" title="mlc_chat.ChatModule.reset_chat"><code class="xref py py-func docutils literal notranslate"><span class="pre">reset_chat()</span></code></a>.
To reload module, please either re-initialize a <a class="reference internal" href="#mlc_chat.ChatModule" title="mlc_chat.ChatModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">ChatModule</span></code></a> instance
or use <code class="xref py py-func docutils literal notranslate"><span class="pre">_reload()</span></code> instead.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mlc_chat.ChatModule.stats">
<span class="sig-name descname"><span class="pre">stats</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#mlc_chat.ChatModule.stats" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the runtime stats of the encoding step, decoding step, (and embedding step if exists)
of the chat module in text form.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>stats</strong> – The runtime stats text.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>str</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="gradio-frontend">
<h2><a class="toc-backref" href="#id7" role="doc-backlink">Gradio Frontend</a><a class="headerlink" href="#gradio-frontend" title="Permalink to this heading">¶</a></h2>
<p>The gradio frontend provides a web interface for the MLC-Chat model, which allows user to interact with the model in a more user-friendly way and switch between different models to compare performance.
To use gradio frontend, you need to install gradio first:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>gradio
</pre></div>
</div>
<p>Then you can run the following code to start the interface:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>mlc_chat.gradio<span class="w"> </span>--artifact-path<span class="w"> </span>ARTIFACT_PATH<span class="w"> </span><span class="o">[</span>--device<span class="w"> </span>DEVICE<span class="o">]</span><span class="w"> </span><span class="o">[</span>--port<span class="w"> </span>PORT_NUMBER<span class="o">]</span><span class="w"> </span><span class="o">[</span>--share<span class="o">]</span>
</pre></div>
</div>
<dl class="option-list">
<dt><kbd><span class="option">--artifact-path</span></kbd></dt>
<dd><p>Please provide a path containing all the model folders you wish to use. The default value is <code class="docutils literal notranslate"><span class="pre">dist</span></code>.</p>
</dd>
<dt><kbd><span class="option">--device</span></kbd></dt>
<dd><p>The description of the device to run on. User should provide a string in the form of ‘device_name:device_id’ or ‘device_name’, where ‘device_name’ is one of ‘cuda’, ‘metal’, ‘vulkan’, ‘rocm’, ‘opencl’, ‘auto’ (automatically detect the local device), and ‘device_id’ is the device id to run on. If no ‘device_id’ is provided, it will be set to 0. The default value is <code class="docutils literal notranslate"><span class="pre">auto</span></code>.</p>
</dd>
<dt><kbd><span class="option">--port</span></kbd></dt>
<dd><p>The port number to run gradio. The default value is <code class="docutils literal notranslate"><span class="pre">7860</span></code>.</p>
</dd>
<dt><kbd><span class="option">--share</span></kbd></dt>
<dd><p>Whether to create a publicly shareable link for the interface.</p>
</dd>
</dl>
<p>After setting up properly, you are expected to see the following interface in your browser:</p>
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/gradio-interface.png"><img alt="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/gradio-interface.png" class="align-center" src="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/gradio-interface.png" style="width: 100%;" /></a>
</section>
</section>


           </div>
           
          </div>
          

<footer>

    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="ios.html" class="btn btn-neutral float-right" title="iOS App and Swift API" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="cli.html" class="btn btn-neutral float-left" title="CLI and C++ API" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>

<div id="button" class="backtop"><img src="../_static/img/right.svg" alt="backtop"/> </div>
<section class="footerSec">
    <div class="footerHeader">
      <div class="d-flex align-md-items-center justify-content-between flex-column flex-md-row">
        <div class="copywrite d-flex align-items-center">
          <h5 id="copy-right-info">© 2023 MLC LLM</h5>
        </div>
      </div>

    </div>

    <div>
      <div class="footernote"> </div>
    </div>

</section>
</footer>
        </div>
      </div>

    </section>

  </div>
  

    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

  </body>
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>