





<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Configure MLCChat in JSON &mdash; mlc-llm 0.1.0 documentation</title>
  

  
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/tabs.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/tlcpack_theme.css" type="text/css" />

  
  

  
  
  
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <script type="text/javascript" src="../_static/js/tlcpack_theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="WebLLM and Javascript API" href="../deploy/javascript.html" />
    <link rel="prev" title="Project Overview" href="project_overview.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    
<header class="header">
    <div class="innercontainer">
      <div class="headerInner d-flex justify-content-between align-items-center">
          <div class="headerLogo">
          </div>

          <div id="headMenu" class="headerNav">
            <button type="button" id="closeHeadMenu" class="navCloseBtn"><img src="../_static/img/close-icon.svg" alt="Close"></button>
             <ul class="nav">
                <li class="nav-item">
                   <a class="nav-link" href=https://mlc.ai/mlc-llm>Home</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://github.com/mlc-ai/mlc-llm>Github</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://discord.gg/9Xpy2HGBuD>Discord Server</a>
                </li>
             </ul>
               <div class="responsivetlcdropdown">
                 <button type="button" class="btn-link">
                   Other Resources
                 </button>
                 <ul>
                     <li>
                       <a href=https://mlc.ai/>MLC Course</a>
                     </li>
                     <li>
                       <a href=https://mlc.ai/blog>MLC Blog</a>
                     </li>
                     <li>
                       <a href=https://webllm.mlc.ai/>Web LLM</a>
                     </li>
                 </ul>
               </div>
          </div>
            <div class="responsiveMenuIcon">
              <button type="button" id="menuBtn" class="btn-menu"><img src="../_static/img/menu-icon.svg" alt="Menu Icon"></button>
            </div>

            <div class="tlcDropdown">
              <div class="dropdown">
                <button type="button" class="btn-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                  Other Resources
                </button>
                <div class="dropdown-menu dropdown-menu-right">
                  <ul>
                     <li>
                       <a href=https://mlc.ai/>MLC Course</a>
                     </li>
                     <li>
                       <a href=https://mlc.ai/blog>MLC Blog</a>
                     </li>
                     <li>
                       <a href=https://webllm.mlc.ai/>Web LLM</a>
                     </li>
                  </ul>
                </div>
              </div>
          </div>
       </div>
    </div>
 </header>
 
    <nav data-toggle="wy-nav-shift" class="wy-nav-side fixed">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/mlc-logo-with-text-landscape.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
                <div class="version">
                  0.1.0
                </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="try_out.html">Try out MLC Chat</a></li>
<li class="toctree-l1"><a class="reference internal" href="project_overview.html">Project Overview</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Configure MLCChat in JSON</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#structure-of-mlcchat-configuration">Structure of MLCChat Configuration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#conversation-structure">Conversation Structure</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#load-from-pre-defined-conversation-templates">Load from Pre-defined Conversation Templates</a></li>
<li class="toctree-l2"><a class="reference internal" href="#load-from-json-conversation-configuration">Load from JSON Conversation Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="#customize-conversation-template">Customize Conversation Template</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#example-1-replace-system-prompt">Example 1: Replace System Prompt</a></li>
<li class="toctree-l3"><a class="reference internal" href="#example-2-resume-from-chat-history">Example 2: Resume from Chat History</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Build and Deploy Apps</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../deploy/javascript.html">WebLLM and Javascript API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/rest.html">Rest API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/cli.html">CLI and C++ API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/python.html">Python API and Gradio Frontend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/ios.html">iOS App and Swift API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/android.html">Android App</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Compile Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../compilation/compile_models.html">Compile Models via MLC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compilation/distribute_compiled_models.html">Distribute Compiled Models</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Define Model Architectures</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/customize/define_new_models.html">Define New Model Architectures</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Prebuilt Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../prebuilt_models.html">Model Prebuilts</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Dependency Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install/tvm.html">Install TVM Unity</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/conda.html">Install Conda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/gpu.html">GPU Drivers and SDKs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/emcc.html">Install Wasm Build Environment</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../community/guideline.html">Community Guideline</a></li>
<li class="toctree-l1"><a class="reference internal" href="../community/faq.html">Frequently Asked Questions</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      
      <nav class="wy-nav-top" aria-label="top navigation" data-toggle="wy-nav-top">
        
            <div class="togglemenu">

            </div>
            <div class="nav-content">
              <!-- mlc-llm -->
              Table of Contents
            </div>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        

          




















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> <span class="br-arrow">></span></li>
        
      <li>Configure MLCChat in JSON</li>
    
    
      
      
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/mlc-ai/mlc-llm/edit/main/docs/get_started/mlc_chat_config.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="configure-mlcchat-in-json">
<span id="configure-mlc-chat-json"></span><h1>Configure MLCChat in JSON<a class="headerlink" href="#configure-mlcchat-in-json" title="Permalink to this heading">¶</a></h1>
<p>This page explains the components of a chat configuration and how to customize them for your own purposes.</p>
<p>Each MLC Chat runtime can be configured via an <code class="docutils literal notranslate"><span class="pre">mlc-chat-config.json</span></code> file under the directory of each compiled model (e.g.
<a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-RedPajama-INCITE-Chat-3B-v1-q4f16_1/blob/main/mlc-chat-config.json">RedPajama chat config</a>)
which contains the chat configuration. You can customize the chat configuration by modifying this file.
Additionally, the runtimes also provide APIs to optionally override some of the configurations.</p>
<section id="structure-of-mlcchat-configuration">
<span id="struct-mlc-chat-conv"></span><h2>Structure of MLCChat Configuration<a class="headerlink" href="#structure-of-mlcchat-configuration" title="Permalink to this heading">¶</a></h2>
<p>Below is the <code class="docutils literal notranslate"><span class="pre">mlc-chat-config.json</span></code> file corresponding to Llama2 model:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="c1">// mlc-chat-config.json</span>
<span class="p">{</span>
<span class="w">  </span><span class="nt">&quot;model_lib&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Llama-2-7b-chat-hf-q4f16_1&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;local_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Llama-2-7b-chat-hf-q4f16_1&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;conv_template&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;llama-2&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;temperature&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">0.7</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;repetition_penalty&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">1.0</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;top_p&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">0.95</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;mean_gen_len&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;max_gen_len&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">512</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;shift_fill_factor&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">0.3</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;tokenizer_files&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">      </span><span class="s2">&quot;added_tokens.json&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s2">&quot;tokenizer.json&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s2">&quot;tokenizer.model&quot;</span>
<span class="w">  </span><span class="p">],</span>
<span class="w">  </span><span class="nt">&quot;model_category&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;llama&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;model_name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Llama-2-7b-chat-hf&quot;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The following fields contain the meta-data which affect system behaviors.</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">model_lib</span></code></dt><dd><p>The necessary model library to launch this model architecture. We recommend reuse model library when possible.
For example, all LLaMA-7B models compiled under <code class="docutils literal notranslate"><span class="pre">q4f16_1</span></code> quantization
can use <cite>Llama-2-7b-chat-hf-q4f16_1</cite>. So you can distribute LLaMA-7B
weight variants and still use them in prebuilt MLC chat apps.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">local_id</span></code></dt><dd><p>Uniquely identifying the model in application. This is also used by command line interface app to specify which model to run.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">tokenizer_files</span></code></dt><dd><p>List of tokenizer files of the model.</p>
</dd>
</dl>
<p>The following fields can be customized to change the behavior of the model:</p>
<dl>
<dt><code class="docutils literal notranslate"><span class="pre">conv_template</span></code></dt><dd><p>The name of the conversation template that this chat uses. For more information, please refer to <a class="reference internal" href="#struct-conv"><span class="std std-ref">conversation structure</span></a>.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">temperature</span></code></dt><dd><p>The temperature applied to logits before sampling. The default value is <code class="docutils literal notranslate"><span class="pre">0.7</span></code>. A higher temperature encourages more diverse outputs, while a lower temperature produces more deterministic outputs.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">repetition_penalty</span></code></dt><dd><p>The repetition penalty controls the likelihood of the model generating repeated texts. The default value is set to <code class="docutils literal notranslate"><span class="pre">1.0</span></code>, indicating that no repetition penalty is applied. Increasing the value reduces the likelihood of repeat text generation. However, setting a high <code class="docutils literal notranslate"><span class="pre">repetition_penalty</span></code> may result in the model generating meaningless texts. The ideal choice of repetition penalty may vary among models.</p>
<p>For more details on how repetition penalty controls text generation, please check out the <a class="reference external" href="https://arxiv.org/pdf/1909.05858.pdf">CTRL paper</a>.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">top_p</span></code></dt><dd><p>This parameter determines the set of tokens from which we sample during decoding. The default value is set to <code class="docutils literal notranslate"><span class="pre">0.95</span></code>. At each step, we select tokens from the minimal set that has a cumulative probability exceeding the <code class="docutils literal notranslate"><span class="pre">top_p</span></code> parameter.</p>
<p>For additional information on top-p sampling, please refer to this <a class="reference external" href="https://huggingface.co/blog/how-to-generate#top-p-nucleus-sampling">blog post</a>.</p>
</dd>
</dl>
<section id="conversation-structure">
<span id="struct-conv"></span><h3>Conversation Structure<a class="headerlink" href="#conversation-structure" title="Permalink to this heading">¶</a></h3>
<p>There are three options of loading conversation configurations:</p>
<ol class="arabic simple">
<li><p>Load from pre-defined conversation templates.</p></li>
<li><p>Load from JSON format conversation configuration.</p></li>
<li><p>First load from pre-defined conversation templates, then override some fields with JSON format conversation configuration.</p></li>
</ol>
</section>
</section>
<section id="load-from-pre-defined-conversation-templates">
<span id="load-predefined-conv-template"></span><h2>Load from Pre-defined Conversation Templates<a class="headerlink" href="#load-from-pre-defined-conversation-templates" title="Permalink to this heading">¶</a></h2>
<p>MLC-LLM provided a set of pre-defined conversation templates, which you can directly use by specifying the template name in <code class="docutils literal notranslate"><span class="pre">conv_template</span></code> field in the <code class="docutils literal notranslate"><span class="pre">mlc-chat-config.json</span></code>, below is a list (not complete) of supported conversation templates:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">llama-2</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">vicuna_v1.1</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">redpajama_chat</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">rwkv</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dolly</span></code></p></li>
<li><p>…</p></li>
</ul>
<p>Please refer to <a class="reference external" href="https://github.com/mlc-ai/mlc-llm/blob/main/cpp/conv_templates.cc">conv_template.cc</a> for the full list of supported templates and their implementations.</p>
</section>
<section id="load-from-json-conversation-configuration">
<span id="load-json-conv-config"></span><h2>Load from JSON Conversation Configuration<a class="headerlink" href="#load-from-json-conversation-configuration" title="Permalink to this heading">¶</a></h2>
<p>Below is a generic structure of a JSON conversation configuration (we use vicuna as an example):</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="c1">// mlc-chat-config.json</span>
<span class="p">{</span>
<span class="w">  </span><span class="c1">// ...</span>
<span class="w">  </span><span class="nt">&quot;conv_config&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;seps&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">      </span><span class="s2">&quot; &quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s2">&quot;&lt;\/s&gt;&quot;</span>
<span class="w">    </span><span class="p">],</span>
<span class="w">    </span><span class="nt">&quot;stop_tokens&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">      </span><span class="mi">2</span>
<span class="w">    </span><span class="p">],</span>
<span class="w">    </span><span class="nt">&quot;offset&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;separator_style&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;messages&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[],</span>
<span class="w">    </span><span class="nt">&quot;stop_str&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;&lt;\/s&gt;&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;roles&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">      </span><span class="s2">&quot;USER&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="s2">&quot;ASSISTANT&quot;</span>
<span class="w">    </span><span class="p">],</span>
<span class="w">    </span><span class="nt">&quot;role_msg_sep&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;: &quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;role_empty_sep&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;: &quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;system&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user&#39;s questions.&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;add_bos&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;vicuna_v1.1&quot;</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">roles</span></code></dt><dd><p>An array that describes the role names of the user and the model. These names are specific to the model being used.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">system</span></code></dt><dd><p>The prompt encoded before starting the chat. It can be customized to a user-defined prompt.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">add_bos</span></code></dt><dd><p>Determines whether a beginning-of-string (bos) token should be added before the input tokens.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">stop_str</span></code></dt><dd><p>When the <code class="docutils literal notranslate"><span class="pre">stop_str</span></code> is encountered, the model will stop generating output.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">stop_tokens</span></code></dt><dd><p>A list of token IDs that act as stop tokens.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">seps</span></code></dt><dd><p>An array of strings indicating the separators to be used after a user message and a model message respectively.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">messages</span></code></dt><dd><p>The chat history represented as an array of string pairs in the following format: <code class="docutils literal notranslate"><span class="pre">[[role_0,</span> <span class="pre">msg_0],</span> <span class="pre">[role_1,</span> <span class="pre">msg_1],</span> <span class="pre">...]</span></code></p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">offset</span></code></dt><dd><p>The offset used to begin the chat from the chat history. When <code class="docutils literal notranslate"><span class="pre">offset</span></code> is not <code class="docutils literal notranslate"><span class="pre">0</span></code>, <code class="docutils literal notranslate"><span class="pre">messages[0:offset-1]</span></code> will be encoded.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">separator_style</span></code></dt><dd><p>Specifies whether we are in chat-bot mode (<code class="docutils literal notranslate"><span class="pre">0</span></code>) or pure LM prompt mode (<code class="docutils literal notranslate"><span class="pre">1</span></code>).</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">role_msg_sep</span></code></dt><dd><p>A string indicating the separator between a role and a message.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">role_empty_sep</span></code></dt><dd><p>A string indicating the separator to append to a role when there is no message yet.</p>
</dd>
</dl>
<p>When the value of <code class="docutils literal notranslate"><span class="pre">separator_style</span></code> is set to 0 (or <code class="docutils literal notranslate"><span class="pre">kSepRoleMsg</span></code>), each round of conversation follows the format:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>{role[0]}{separator_style}{user_input}{sep[0]}
{role[1]}{separator_style}{model_output}{sep[1]}
</pre></div>
</div>
<p>Here, <code class="docutils literal notranslate"><span class="pre">{user_input}</span></code> represents the input provided by the user, and <code class="docutils literal notranslate"><span class="pre">{model_output}</span></code> represents the output generated by the model.</p>
<p>On the other hand, if the value of <code class="docutils literal notranslate"><span class="pre">separator_style</span></code> is set to 1 (or <code class="docutils literal notranslate"><span class="pre">kLM</span></code>), the model is not aware of the chat history and generates the response immediately after the user input prompt:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>{user_prompt}{model_output}
</pre></div>
</div>
</section>
<section id="customize-conversation-template">
<span id="customize-conv-template"></span><h2>Customize Conversation Template<a class="headerlink" href="#customize-conversation-template" title="Permalink to this heading">¶</a></h2>
<p>In the <code class="docutils literal notranslate"><span class="pre">mlc-chat-config.json</span></code> file, you have the option to specify both <code class="docutils literal notranslate"><span class="pre">conv_template</span></code> and <code class="docutils literal notranslate"><span class="pre">conv_config</span></code>. MLC-LLM will first load the predefined template with the name specified in <code class="docutils literal notranslate"><span class="pre">conv_template</span></code> and then override some of the configurations specified in <code class="docutils literal notranslate"><span class="pre">conv_config</span></code>. It’s important to note that the configurations in <code class="docutils literal notranslate"><span class="pre">conv_config</span></code> don’t need to be complete, allowing for partial updates.</p>
<section id="example-1-replace-system-prompt">
<span id="example-replace-system-prompt"></span><h3>Example 1: Replace System Prompt<a class="headerlink" href="#example-1-replace-system-prompt" title="Permalink to this heading">¶</a></h3>
<p>If you’re tired of the default system prompt, here’s an example of how you can replace it:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="c1">// mlc-chat-config.json</span>
<span class="p">{</span>
<span class="w">  </span><span class="c1">// ...</span>
<span class="w">  </span><span class="nt">&quot;conv_template&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;vicuna_v1.1&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;conv_config&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;system&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;You are not Vicuna, your name is Guanaco, now let&#39;s chat!&quot;</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The next time you run <code class="docutils literal notranslate"><span class="pre">mlc_chat_cli</span></code>, you will start a chat with Vicuna using a new system prompt.</p>
</section>
<section id="example-2-resume-from-chat-history">
<span id="example-resume-chat-history"></span><h3>Example 2: Resume from Chat History<a class="headerlink" href="#example-2-resume-from-chat-history" title="Permalink to this heading">¶</a></h3>
<p>The following example demonstrates how to chat with Vicuna and resume from a chat history:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="c1">// mlc-chat-config.json</span>
<span class="p">{</span>
<span class="w">  </span><span class="c1">// ...</span>
<span class="w">  </span><span class="nt">&quot;conv_template&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;vicuna_v1.1&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;conv_config&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;messages&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">      </span><span class="p">[</span><span class="s2">&quot;USER&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;Suppose we already have projects llama, alpaca and vicuna, what do you think would be a great name for the next project?&quot;</span><span class="p">],</span>
<span class="w">      </span><span class="p">[</span><span class="s2">&quot;ASSISTANT&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;Based on the previous projects, a possible name for the next project could be \&quot;cervidae\&quot; which is the scientific name for deer family. This name reflects the collaboration and teamwork involved in the development of the project, and also nods to the previous projects that have been developed by the team.&quot;</span><span class="p">],</span>
<span class="w">      </span><span class="p">[</span><span class="s2">&quot;USER&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;I like cervidae, but the name is too long!&quot;</span><span class="p">],</span>
<span class="w">      </span><span class="p">[</span><span class="s2">&quot;ASSISTANT&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;In that case, a shorter and catchier name for the next project could be \&quot;DeerRun\&quot; which plays on the idea of the project being fast and efficient, just like a deer running through the woods. This name is memorable and easy to pronounce, making it a good choice for a project name.&quot;</span><span class="p">]</span>
<span class="w">    </span><span class="p">],</span>
<span class="w">    </span><span class="nt">&quot;offset&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">4</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The next time you start <code class="docutils literal notranslate"><span class="pre">mlc_chat_cli</span></code>, you will initiate a chat with Vicuna and resume from the provided chat history.</p>
</section>
</section>
</section>


           </div>
           
          </div>
          

<footer>

    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../deploy/javascript.html" class="btn btn-neutral float-right" title="WebLLM and Javascript API" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="project_overview.html" class="btn btn-neutral float-left" title="Project Overview" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>

<div id="button" class="backtop"><img src="../_static/img/right.svg" alt="backtop"/> </div>
<section class="footerSec">
    <div class="footerHeader">
      <div class="d-flex align-md-items-center justify-content-between flex-column flex-md-row">
        <div class="copywrite d-flex align-items-center">
          <h5 id="copy-right-info">© 2023 MLC LLM</h5>
        </div>
      </div>

    </div>

    <div>
      <div class="footernote"> </div>
    </div>

</section>
</footer>
        </div>
      </div>

    </section>

  </div>
  

    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

  </body>
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>