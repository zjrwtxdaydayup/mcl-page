





<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Model Prebuilts &mdash; mlc-llm 0.1.0 documentation</title>
  

  
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/tabs.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/tlcpack_theme.css" type="text/css" />

  
  

  
  
  
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/sphinx_highlight.js"></script>
        <script src="_static/tabs.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <script type="text/javascript" src="_static/js/tlcpack_theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Install TVM Unity" href="install/tvm.html" />
    <link rel="prev" title="Define New Model Architectures" href="tutorials/customize/define_new_models.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    
<header class="header">
    <div class="innercontainer">
      <div class="headerInner d-flex justify-content-between align-items-center">
          <div class="headerLogo">
          </div>

          <div id="headMenu" class="headerNav">
            <button type="button" id="closeHeadMenu" class="navCloseBtn"><img src="_static/img/close-icon.svg" alt="Close"></button>
             <ul class="nav">
                <li class="nav-item">
                   <a class="nav-link" href=https://mlc.ai/mlc-llm>Home</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://github.com/mlc-ai/mlc-llm>Github</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://discord.gg/9Xpy2HGBuD>Discord Server</a>
                </li>
             </ul>
               <div class="responsivetlcdropdown">
                 <button type="button" class="btn-link">
                   Other Resources
                 </button>
                 <ul>
                     <li>
                       <a href=https://mlc.ai/>MLC Course</a>
                     </li>
                     <li>
                       <a href=https://mlc.ai/blog>MLC Blog</a>
                     </li>
                     <li>
                       <a href=https://webllm.mlc.ai/>Web LLM</a>
                     </li>
                 </ul>
               </div>
          </div>
            <div class="responsiveMenuIcon">
              <button type="button" id="menuBtn" class="btn-menu"><img src="_static/img/menu-icon.svg" alt="Menu Icon"></button>
            </div>

            <div class="tlcDropdown">
              <div class="dropdown">
                <button type="button" class="btn-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                  Other Resources
                </button>
                <div class="dropdown-menu dropdown-menu-right">
                  <ul>
                     <li>
                       <a href=https://mlc.ai/>MLC Course</a>
                     </li>
                     <li>
                       <a href=https://mlc.ai/blog>MLC Blog</a>
                     </li>
                     <li>
                       <a href=https://webllm.mlc.ai/>Web LLM</a>
                     </li>
                  </ul>
                </div>
              </div>
          </div>
       </div>
    </div>
 </header>
 
    <nav data-toggle="wy-nav-shift" class="wy-nav-side fixed">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html">
          

          
            
            <img src="_static/mlc-logo-with-text-landscape.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
                <div class="version">
                  0.1.0
                </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="get_started/try_out.html">Try out MLC Chat</a></li>
<li class="toctree-l1"><a class="reference internal" href="get_started/project_overview.html">Project Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="get_started/mlc_chat_config.html">Configure MLCChat in JSON</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Build and Deploy Apps</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="deploy/javascript.html">WebLLM and Javascript API</a></li>
<li class="toctree-l1"><a class="reference internal" href="deploy/rest.html">Rest API</a></li>
<li class="toctree-l1"><a class="reference internal" href="deploy/cli.html">CLI and C++ API</a></li>
<li class="toctree-l1"><a class="reference internal" href="deploy/python.html">Python API and Gradio Frontend</a></li>
<li class="toctree-l1"><a class="reference internal" href="deploy/ios.html">iOS App and Swift API</a></li>
<li class="toctree-l1"><a class="reference internal" href="deploy/android.html">Android App</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Compile Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="compilation/compile_models.html">Compile Models via MLC</a></li>
<li class="toctree-l1"><a class="reference internal" href="compilation/distribute_compiled_models.html">Distribute Compiled Models</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Define Model Architectures</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tutorials/customize/define_new_models.html">Define New Model Architectures</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Prebuilt Models</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Model Prebuilts</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#prebuilt-models-for-cli">Prebuilt Models for CLI</a></li>
<li class="toctree-l2"><a class="reference internal" href="#prebuilt-models-for-ios">Prebuilt Models for iOS</a></li>
<li class="toctree-l2"><a class="reference internal" href="#prebuilt-models-for-android">Prebuilt Models for Android</a></li>
<li class="toctree-l2"><a class="reference internal" href="#supported-model-architectures">Supported Model Architectures</a></li>
<li class="toctree-l2"><a class="reference internal" href="#contribute-models-to-mlc-llm">Contribute Models to MLC-LLM</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Dependency Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="install/tvm.html">Install TVM Unity</a></li>
<li class="toctree-l1"><a class="reference internal" href="install/conda.html">Install Conda</a></li>
<li class="toctree-l1"><a class="reference internal" href="install/gpu.html">GPU Drivers and SDKs</a></li>
<li class="toctree-l1"><a class="reference internal" href="install/emcc.html">Install Wasm Build Environment</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="community/guideline.html">Community Guideline</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/faq.html">Frequently Asked Questions</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      
      <nav class="wy-nav-top" aria-label="top navigation" data-toggle="wy-nav-top">
        
            <div class="togglemenu">

            </div>
            <div class="nav-content">
              <!-- mlc-llm -->
              Table of Contents
            </div>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        

          




















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> <span class="br-arrow">></span></li>
        
      <li>Model Prebuilts</li>
    
    
      
      
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/mlc-ai/mlc-llm/edit/main/docs/prebuilt_models.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="model-prebuilts">
<span id="id1"></span><h1>Model Prebuilts<a class="headerlink" href="#model-prebuilts" title="Permalink to this heading">¶</a></h1>
<nav class="contents local" id="table-of-contents">
<p class="topic-title">Table of Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#prebuilt-models-for-cli" id="id8">Prebuilt Models for CLI</a></p></li>
<li><p><a class="reference internal" href="#prebuilt-models-for-ios" id="id9">Prebuilt Models for iOS</a></p></li>
<li><p><a class="reference internal" href="#prebuilt-models-for-android" id="id10">Prebuilt Models for Android</a></p></li>
<li><p><a class="reference internal" href="#supported-model-architectures" id="id11">Supported Model Architectures</a></p></li>
<li><p><a class="reference internal" href="#contribute-models-to-mlc-llm" id="id12">Contribute Models to MLC-LLM</a></p></li>
</ul>
</nav>
<p>MLC-LLM is a universal solution for deploying different language models. Any language models that can be described in <a class="reference external" href="https://mlc.ai/chapter_graph_optimization/index.html">TVM Relax</a> (a general representation for Neural Networks and can be imported from models written in PyTorch) can be recognized by MLC-LLM and thus deployed to different backends with the help of <a class="reference internal" href="install/tvm.html"><span class="doc">TVM Unity</span></a>.</p>
<p>The community has already supported several LLM architectures (LLaMA, GPT-NeoX, etc.) and have prebuilt some models (Vicuna, RedPajama, etc.) which you can use off the shelf.
With the goal of democratizing the deployment of LLMs, we eagerly anticipate further contributions from the community to expand the range of supported model architectures.</p>
<p>This page contains the list of prebuilt models for our CLI (command line interface) app, iOS and Android apps.
The models have undergone extensive testing on various devices, and their performance has been optimized by developers with the help of TVM.</p>
<section id="prebuilt-models-for-cli">
<span id="prebuilt-models-cli"></span><h2><a class="toc-backref" href="#id8" role="doc-backlink">Prebuilt Models for CLI</a><a class="headerlink" href="#prebuilt-models-for-cli" title="Permalink to this heading">¶</a></h2>
<table class="docutils align-default">
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Model code</p></th>
<th class="head"><p>Model Series</p></th>
<th class="head"><p>Quantization Mode</p></th>
<th class="head"><p>Hugging Face repo</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><cite>Llama-2-7b-q4f16_1</cite></p></td>
<td><p><a class="reference external" href="https://ai.meta.com/llama/">Llama</a></p></td>
<td><ul class="simple">
<li><p>Weight storage data type: int4</p></li>
<li><p>Running data type: float16</p></li>
<li><p>Symmetric quantization</p></li>
</ul>
</td>
<td><p><a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-Llama-2-7b-chat-hf-q4f16_1">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><cite>vicuna-v1-7b-q3f16_0</cite></p></td>
<td><p><a class="reference external" href="https://lmsys.org/blog/2023-03-30-vicuna/">Vicuna</a></p></td>
<td><ul class="simple">
<li><p>Weight storage data type: int3</p></li>
<li><p>Running data type: float16</p></li>
<li><p>Symmetric quantization</p></li>
</ul>
</td>
<td><p><a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-vicuna-v1-7b-q3f16_0">link</a></p></td>
</tr>
<tr class="row-even"><td><p><cite>RedPajama-INCITE-Chat-3B-v1-q4f16_1</cite></p></td>
<td><p><a class="reference external" href="https://www.together.xyz/blog/redpajama">RedPajama</a></p></td>
<td><ul class="simple">
<li><p>Weight storage data type: int4</p></li>
<li><p>Running data type: float16</p></li>
<li><p>Symmetric quantization</p></li>
</ul>
</td>
<td><p><a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-RedPajama-INCITE-Chat-3B-v1-q4f16_1">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><cite>rwkv-raven-1b5-q8f16_0</cite></p></td>
<td><p><a class="reference external" href="https://github.com/BlinkDL/RWKV-LM">RWKV</a></p></td>
<td><ul class="simple">
<li><p>Weight storage data type: uint8</p></li>
<li><p>Running data type: float16</p></li>
<li><p>Symmetric quantization</p></li>
</ul>
</td>
<td><p><a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-rwkv-raven-1b5-q8f16_0">link</a></p></td>
</tr>
<tr class="row-even"><td><p><cite>rwkv-raven-3b-q8f16_0</cite></p></td>
<td><p><a class="reference external" href="https://github.com/BlinkDL/RWKV-LM">RWKV</a></p></td>
<td><ul class="simple">
<li><p>Weight storage data type: uint8</p></li>
<li><p>Running data type: float16</p></li>
<li><p>Symmetric quantization</p></li>
</ul>
</td>
<td><p><a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-rwkv-raven-3b-q8f16_0">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><cite>rwkv-raven-7b-q8f16_0</cite></p></td>
<td><p><a class="reference external" href="https://github.com/BlinkDL/RWKV-LM">RWKV</a></p></td>
<td><ul class="simple">
<li><p>Weight storage data type: uint8</p></li>
<li><p>Running data type: float16</p></li>
<li><p>Symmetric quantization</p></li>
</ul>
</td>
<td><p><a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-rwkv-raven-7b-q8f16_0">link</a></p></td>
</tr>
</tbody>
</table>
<p>To download and run one model with CLI, follow the instructions below:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create conda environment and install CLI if you have not installed.</span>
conda<span class="w"> </span>create<span class="w"> </span>-n<span class="w"> </span>mlc-chat-venv<span class="w"> </span>-c<span class="w"> </span>mlc-ai<span class="w"> </span>-c<span class="w"> </span>conda-forge<span class="w"> </span>mlc-chat-cli-nightly
conda<span class="w"> </span>activate<span class="w"> </span>mlc-chat-venv
conda<span class="w"> </span>install<span class="w"> </span>git<span class="w"> </span>git-lfs
git<span class="w"> </span>lfs<span class="w"> </span>install

<span class="c1"># Download prebuilt model binary libraries from GitHub if you have not downloaded.</span>
mkdir<span class="w"> </span>-p<span class="w"> </span>dist/prebuilt
git<span class="w"> </span>clone<span class="w"> </span>https://github.com/mlc-ai/binary-mlc-llm-libs.git<span class="w"> </span>dist/prebuilt/lib

<span class="c1"># Download prebuilt model weights and run CLI.</span>
<span class="nb">cd</span><span class="w"> </span>dist/prebuilt
git<span class="w"> </span>clone<span class="w"> </span>https://huggingface.co/mlc-ai/mlc-chat-<span class="o">[</span>model-code<span class="o">]</span>
<span class="nb">cd</span><span class="w"> </span>../..
mlc_chat_cli<span class="w"> </span>--local-id<span class="w"> </span><span class="o">[</span>model-code<span class="o">]</span>

<span class="c1"># e.g.,</span>
<span class="c1"># cd dist/prebuilt</span>
<span class="c1"># git clone https://huggingface.co/mlc-ai/mlc-chat-rwkv-raven-7b-q8f16_0</span>
<span class="c1"># cd ../..</span>
<span class="c1"># mlc_chat_cli --local-id rwkv-raven-7b-q8f16_0</span>
</pre></div>
</div>
</section>
<section id="prebuilt-models-for-ios">
<span id="prebuilt-models-ios"></span><h2><a class="toc-backref" href="#id9" role="doc-backlink">Prebuilt Models for iOS</a><a class="headerlink" href="#prebuilt-models-for-ios" title="Permalink to this heading">¶</a></h2>
<table class="docutils align-default" id="id4">
<caption><span class="caption-text">Prebuilt models for iOS</span><a class="headerlink" href="#id4" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Model code</p></th>
<th class="head"><p>Model Series</p></th>
<th class="head"><p>Quantization Mode</p></th>
<th class="head"><p>Hugging Face repo</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><cite>Llama-2-7b-q3f16_1</cite></p></td>
<td><p><a class="reference external" href="https://ai.meta.com/llama/">Llama</a></p></td>
<td><ul class="simple">
<li><p>Weight storage data type: int3</p></li>
<li><p>Running data type: float16</p></li>
<li><p>Symmetric quantization</p></li>
</ul>
</td>
<td><p><a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-Llama-2-7b-chat-hf-q3f16_1">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><cite>vicuna-v1-7b-q3f16_0</cite></p></td>
<td><p><a class="reference external" href="https://lmsys.org/blog/2023-03-30-vicuna/">Vicuna</a></p></td>
<td><ul class="simple">
<li><p>Weight storage data type: int3</p></li>
<li><p>Running data type: float16</p></li>
<li><p>Symmetric quantization</p></li>
</ul>
</td>
<td><p><a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-vicuna-v1-7b-q3f16_0">link</a></p></td>
</tr>
<tr class="row-even"><td><p><cite>RedPajama-INCITE-Chat-3B-v1-q4f16_1</cite></p></td>
<td><p><a class="reference external" href="https://www.together.xyz/blog/redpajama">RedPajama</a></p></td>
<td><ul class="simple">
<li><p>Weight storage data type: int4</p></li>
<li><p>Running data type: float16</p></li>
<li><p>Symmetric quantization</p></li>
</ul>
</td>
<td><p><a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-RedPajama-INCITE-Chat-3B-v1-q4f16_1">link</a></p></td>
</tr>
</tbody>
</table>
<p>The <a class="reference external" href="https://apps.apple.com/us/app/mlc-chat/id6448482937">downloadable iOS app</a> has builtin RedPajama-3B model support.
To add a model to the iOS app, follow the steps below:</p>
<details class="summary-click-to-show-instructions">
<summary>Click to show instructions</summary><div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-0-0-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-0-0-0" name="0-0" role="tab" tabindex="0">Step 1</button><button aria-controls="panel-0-0-1" aria-selected="false" class="sphinx-tabs-tab" id="tab-0-0-1" name="0-1" role="tab" tabindex="-1">Step 2</button><button aria-controls="panel-0-0-2" aria-selected="false" class="sphinx-tabs-tab" id="tab-0-0-2" name="0-2" role="tab" tabindex="-1">Step 3</button><button aria-controls="panel-0-0-3" aria-selected="false" class="sphinx-tabs-tab" id="tab-0-0-3" name="0-3" role="tab" tabindex="-1">Step 4</button></div><div aria-labelledby="tab-0-0-0" class="sphinx-tabs-panel" id="panel-0-0-0" name="0-0" role="tabpanel" tabindex="0"><p>Open “MLCChat” app, click “Add model variant”.</p>
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/iPhone-custom-1.png"><img alt="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/iPhone-custom-1.png" class="align-center" src="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/iPhone-custom-1.png" style="width: 30%;" /></a>
</div><div aria-labelledby="tab-0-0-1" class="sphinx-tabs-panel" hidden="true" id="panel-0-0-1" name="0-1" role="tabpanel" tabindex="0"><p>Paste the repository URL of the model built on your own, and click “Add”.</p>
<p>You can refer to the link in the image as an example.</p>
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/iPhone-custom-2.png"><img alt="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/iPhone-custom-2.png" class="align-center" src="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/iPhone-custom-2.png" style="width: 30%;" /></a>
</div><div aria-labelledby="tab-0-0-2" class="sphinx-tabs-panel" hidden="true" id="panel-0-0-2" name="0-2" role="tabpanel" tabindex="0"><p>After adding the model, you can download your model from the URL by clicking the download button.</p>
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/iPhone-custom-3.png"><img alt="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/iPhone-custom-3.png" class="align-center" src="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/iPhone-custom-3.png" style="width: 30%;" /></a>
</div><div aria-labelledby="tab-0-0-3" class="sphinx-tabs-panel" hidden="true" id="panel-0-0-3" name="0-3" role="tabpanel" tabindex="0"><p>When the download is finished, click into the model and enjoy.</p>
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/iPhone-custom-4.png"><img alt="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/iPhone-custom-4.png" class="align-center" src="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/iPhone-custom-4.png" style="width: 30%;" /></a>
</div></div>
</details><div class="line-block">
<div class="line"><br /></div>
</div>
<p>The iOS app has integrated with the following model libraries, which can be directly reused when you want to run a model you compiled in iOS, as long as the model is in the supported model family and is compiled with supported quantization mode.
For example, if you compile <a class="reference external" href="https://github.com/openlm-research/open_llama">OpenLLaMA-7B</a> with quantization mode <code class="docutils literal notranslate"><span class="pre">q3f16_0</span></code>, then you can run the compiled OpenLLaMA model on iPhone without rebuilding the iOS app by reusing the <cite>vicuna-v1-7b-q3f16_0</cite> model library. Please check the <a class="reference internal" href="compilation/distribute_compiled_models.html"><span class="doc">model distribution page</span></a> for detailed instructions.</p>
<table class="docutils align-default" id="id5">
<caption><span class="caption-text">Prebuilt model libraries which are integrated in the iOS app</span><a class="headerlink" href="#id5" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Model library name</p></th>
<th class="head"><p>Model Family</p></th>
<th class="head"><p>Quantization Mode</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><cite>vicuna-v1-7b-q3f16_0</cite></p></td>
<td><p>LLaMA</p></td>
<td><ul class="simple">
<li><p>Weight storage data type: int3</p></li>
<li><p>Running data type: float16</p></li>
<li><p>Symmetric quantization</p></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p><cite>RedPajama-INCITE-Chat-3B-v1-q4f16_1</cite></p></td>
<td><p>GPT-NeoX</p></td>
<td><ul class="simple">
<li><p>Weight storage data type: int4</p></li>
<li><p>Running data type: float16</p></li>
<li><p>Symmetric quantization</p></li>
</ul>
</td>
</tr>
</tbody>
</table>
</section>
<section id="prebuilt-models-for-android">
<span id="prebuilt-models-android"></span><h2><a class="toc-backref" href="#id10" role="doc-backlink">Prebuilt Models for Android</a><a class="headerlink" href="#prebuilt-models-for-android" title="Permalink to this heading">¶</a></h2>
<table class="docutils align-default" id="id6">
<caption><span class="caption-text">Prebuilt models for Android</span><a class="headerlink" href="#id6" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Model code</p></th>
<th class="head"><p>Model Series</p></th>
<th class="head"><p>Quantization Mode</p></th>
<th class="head"><p>Hugging Face repo</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><cite>vicuna-v1-7b-q4f16_1</cite></p></td>
<td><p><a class="reference external" href="https://lmsys.org/blog/2023-03-30-vicuna/">Vicuna</a></p></td>
<td><ul class="simple">
<li><p>Weight storage data type: int4</p></li>
<li><p>Running data type: float16</p></li>
<li><p>Symmetric quantization</p></li>
</ul>
</td>
<td><p><a class="reference external" href="https://huggingface.co/mlc-ai/demo-vicuna-v1-7b-int4">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><cite>RedPajama-INCITE-Chat-3B-v1-q4f16_0</cite></p></td>
<td><p><a class="reference external" href="https://www.together.xyz/blog/redpajama">RedPajama</a></p></td>
<td><ul class="simple">
<li><p>Weight storage data type: int4</p></li>
<li><p>Running data type: float16</p></li>
<li><p>Symmetric quantization</p></li>
</ul>
</td>
<td><p><a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-RedPajama-INCITE-Chat-3B-v1-q4f16_0">link</a></p></td>
</tr>
</tbody>
</table>
<hr class="docutils" />
<p>You can check <a class="reference external" href="https://github.com/mlc-ai/mlc-llm/pulls?q=is%3Aopen+is%3Apr+label%3Anew-models">MLC-LLM pull requests</a> to track the ongoing efforts of new models. We encourage users to upload their compiled models to Hugging Face and share with the community.</p>
</section>
<section id="supported-model-architectures">
<span id="id2"></span><h2><a class="toc-backref" href="#id11" role="doc-backlink">Supported Model Architectures</a><a class="headerlink" href="#supported-model-architectures" title="Permalink to this heading">¶</a></h2>
<p>MLC-LLM supports the following model architectures:</p>
<table class="docutils align-default" id="id7">
<caption><span class="caption-text">Supported Model Architectures</span><a class="headerlink" href="#id7" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Category Code</p></th>
<th class="head"><p>Series</p></th>
<th class="head"><p>Model Definition</p></th>
<th class="head"><p>Variants</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">llama</span></code></p></td>
<td><p><a class="reference external" href="https://github.com/facebookresearch/llama">LLaMa</a></p></td>
<td><p><a class="reference external" href="https://github.com/mlc-ai/mlc-llm/blob/main/mlc_llm/relax_model/llama.py">Relax Code</a></p></td>
<td><ul class="simple">
<li><p><a class="reference external" href="https://ai.meta.com/llama/">Llama-2</a></p></li>
<li><p><a class="reference external" href="https://github.com/tatsu-lab/stanford_alpaca">Alpaca</a></p></li>
<li><p><a class="reference external" href="https://lmsys.org/blog/2023-03-30-vicuna/">Vicuna</a></p></li>
<li><p><a class="reference external" href="https://github.com/artidoro/qlora">Guanaco</a></p></li>
<li><p><a class="reference external" href="https://github.com/openlm-research/open_llama">OpenLLaMA</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/gorilla-llm/gorilla-7b-hf-delta-v0">Gorilla</a></p></li>
<li><p><a class="reference external" href="https://github.com/nlpxucan/WizardLM">WizardLM</a></p></li>
<li><p><a class="reference external" href="https://github.com/RUC-GSAI/YuLan-Chat">YuLan-Chat</a></p></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">gpt-neox</span></code></p></td>
<td><p><a class="reference external" href="https://github.com/EleutherAI/gpt-neox">GPT-NeoX</a></p></td>
<td><p><a class="reference external" href="https://github.com/mlc-ai/mlc-llm/blob/main/mlc_llm/relax_model/gpt_neox.py">Relax Code</a></p></td>
<td><ul class="simple">
<li><p><a class="reference external" href="https://www.together.xyz/blog/redpajama">RedPajama</a></p></li>
<li><p><a class="reference external" href="https://github.com/databrickslabs/dolly">Dolly</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/EleutherAI/pythia-1.4b">Pythia</a></p></li>
</ul>
</td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">gptj</span></code></p></td>
<td><p><a class="reference external" href="https://huggingface.co/EleutherAI/gpt-j-6b">GPT-J</a></p></td>
<td><p><a class="reference external" href="https://github.com/mlc-ai/mlc-llm/blob/main/mlc_llm/relax_model/gptj.py">Relax Code</a></p></td>
<td><ul class="simple">
<li><p><a class="reference external" href="https://github.com/OpenLMLab/MOSS">MOSS</a></p></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">rwkv</span></code></p></td>
<td><p><a class="reference external" href="https://github.com/BlinkDL/RWKV-LM">RWKV</a></p></td>
<td><p><a class="reference external" href="https://github.com/mlc-ai/mlc-llm/blob/main/mlc_llm/relax_model/rwkv.py">Relax Code</a></p></td>
<td><ul class="simple">
<li><p><a class="reference external" href="https://github.com/BlinkDL/RWKV-LM">RWKV-raven</a></p></li>
</ul>
</td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">minigpt</span></code></p></td>
<td><p><a class="reference external" href="https://huggingface.co/Vision-CAIR/MiniGPT-4">MiniGPT</a></p></td>
<td><p><a class="reference external" href="https://github.com/mlc-ai/mlc-llm/blob/main/mlc_llm/relax_model/minigpt.py">Relax Code</a></p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">gpt_bigcode</span></code></p></td>
<td><p><a class="reference external" href="https://huggingface.co/docs/transformers/model_doc/gpt_bigcode">GPTBigCode</a></p></td>
<td><p><a class="reference external" href="https://github.com/mlc-ai/mlc-llm/blob/main/mlc_llm/relax_model/gpt_bigcode.py">Relax Code</a></p></td>
<td><ul class="simple">
<li><p><a class="reference external" href="https://huggingface.co/bigcode/starcoder">StarCoder</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/WizardLM/WizardCoder-15B-V1.0">WizardCoder</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/bigcode/gpt_bigcode-santacoder">SantaCoder</a></p></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>For models structured in these model architectures, you can check the <a class="reference internal" href="compilation/compile_models.html"><span class="doc">model compilation page</span></a> on how to compile models.
Please <a class="reference external" href="https://github.com/mlc-ai/mlc-llm/issues/new/choose">create a new issue</a> if you want to request a new model architecture.
Our tutorial <a class="reference internal" href="tutorials/customize/define_new_models.html"><span class="doc">Define New Models</span></a> introduces how to bring a new model architecture to MLC-LLM.</p>
</section>
<section id="contribute-models-to-mlc-llm">
<span id="id3"></span><h2><a class="toc-backref" href="#id12" role="doc-backlink">Contribute Models to MLC-LLM</a><a class="headerlink" href="#contribute-models-to-mlc-llm" title="Permalink to this heading">¶</a></h2>
<p>Ready to contribute your compiled models/new model architectures? Awesome! Please check <a class="reference internal" href="community/guideline.html#contribute-new-models"><span class="std std-ref">Contribute New Models to MLC-LLM</span></a> on how to contribute new models to MLC-LLM.</p>
</section>
</section>


           </div>
           
          </div>
          

<footer>

    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="install/tvm.html" class="btn btn-neutral float-right" title="Install TVM Unity" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="tutorials/customize/define_new_models.html" class="btn btn-neutral float-left" title="Define New Model Architectures" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>

<div id="button" class="backtop"><img src="_static/img/right.svg" alt="backtop"/> </div>
<section class="footerSec">
    <div class="footerHeader">
      <div class="d-flex align-md-items-center justify-content-between flex-column flex-md-row">
        <div class="copywrite d-flex align-items-center">
          <h5 id="copy-right-info">© 2023 MLC LLM</h5>
        </div>
      </div>

    </div>

    <div>
      <div class="footernote"> </div>
    </div>

</section>
</footer>
        </div>
      </div>

    </section>

  </div>
  

    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

  </body>
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>